{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.182  Python-3.9.13 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1050, 2048MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=segment, mode=train, model=c:/3rd_project/yolov8_pt/yolov8n-seg.pt, data=c:/3rd_project/data/Custom/coco_gun-seg/custom-seg.yaml, epochs=1000, patience=20, batch=4, imgsz=448, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\segment\\train10\n",
      "Overriding model.yaml nc=80 with nc=81\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1159309  ultralytics.nn.modules.head.Segment          [81, 32, 64, [64, 128, 256]]  \n",
      "YOLOv8n-seg summary: 261 layers, 3418845 parameters, 3418829 gradients\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\segment\\train10', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\3rd_project\\data\\Custom\\coco_gun-seg\\train\\labels.cache... 4664 images, 38 backgrounds, 0 corrupt: 100%|██████████| 4664/4664 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\3rd_project\\data\\Custom\\coco_gun-seg\\valid\\labels.cache... 1190 images, 18 backgrounds, 0 corrupt: 100%|██████████| 1190/1190 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\segment\\train10\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 448 train, 448 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns\\segment\\train10\u001b[0m\n",
      "Starting training for 1000 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     1/1000     0.843G      1.331       2.89       4.62      1.221         21        448: 100%|██████████| 1166/1166 [04:10<00:00,  4.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 149/149 [00:23<00:00,  6.43it/s]\n",
      "                   all       1190       7573     0.0183     0.0775     0.0211     0.0146     0.0164     0.0717     0.0193     0.0124\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     2/1000     0.789G      1.478      3.079      3.887      1.317         41        448: 100%|██████████| 1166/1166 [03:55<00:00,  4.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 149/149 [00:23<00:00,  6.23it/s]\n",
      "                   all       1190       7573      0.571       0.08     0.0493     0.0301      0.566     0.0731     0.0433     0.0242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     3/1000     0.994G      1.616      3.285      3.576      1.421         41        448: 100%|██████████| 1166/1166 [04:07<00:00,  4.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 149/149 [00:26<00:00,  5.67it/s]\n",
      "                   all       1190       7573      0.316      0.098     0.0491     0.0268       0.33     0.0867     0.0452     0.0222\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     4/1000     0.843G      1.719      3.495      3.427      1.509         35        448: 100%|██████████| 1166/1166 [04:04<00:00,  4.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 149/149 [00:25<00:00,  5.92it/s]\n",
      "                   all       1190       7573      0.292     0.0971     0.0401     0.0227      0.288     0.0866      0.036     0.0186\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     5/1000     0.784G      1.705       3.45      3.303      1.505         37        448: 100%|██████████| 1166/1166 [03:52<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 149/149 [00:25<00:00,  5.86it/s]\n",
      "                   all       1190       7573      0.371      0.115     0.0583     0.0327      0.383      0.104     0.0528      0.027\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     6/1000     0.942G      1.668      3.416      3.195      1.488         19        448: 100%|██████████| 1166/1166 [03:54<00:00,  4.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 149/149 [00:26<00:00,  5.70it/s]\n",
      "                   all       1190       7573      0.345      0.141     0.0716     0.0407      0.349      0.128     0.0645     0.0319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     7/1000     0.967G      1.645      3.372      3.091      1.469         50        448: 100%|██████████| 1166/1166 [03:55<00:00,  4.96it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 149/149 [00:26<00:00,  5.61it/s]\n",
      "                   all       1190       7573      0.369       0.14     0.0808     0.0454       0.36      0.126     0.0708     0.0357\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     8/1000      1.03G      1.619      3.372      3.066       1.46         42        448:  90%|█████████ | 1055/1166 [03:34<00:24,  4.53it/s]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = YOLO('c:/3rd_project/yolov8_pt/yolov8n-seg.pt')\n",
    "\n",
    "    # Train the model\n",
    "    results = model.train(data='c:/3rd_project/data/Custom/coco_gun-seg/custom-seg.yaml', epochs=1000, patience=20, imgsz=448, batch=4)\n",
    "\n",
    "    import torch\n",
    "\n",
    "    # 모델을 저장할 경로 지정\n",
    "    model_save_path = 'c:/3rd_project/yolov8_pt/230927_a.pt'\n",
    "\n",
    "    # 모델 저장\n",
    "    torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\3rd_project\\data\\images\\movie\\action\\411.jpg: 416x288 2 persons, 26.9ms\n",
      "Speed: 0.0ms preprocess, 26.9ms inference, 2.6ms postprocess per image at shape (1, 3, 416, 288)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: person\t\tValue: 2\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "\n",
    "    '''\n",
    "    모델 호출\n",
    "    '''\n",
    "    # # Load a pretrained YOLOv8n model\n",
    "    # model = YOLO('/content/drive/MyDrive/YOLOv8/yolov8n.pt')        # Boxes 모델\n",
    "    # model = YOLO('/content/drive/MyDrive/YOLOv8/yolov8n-seg.pt')   # Mask 모델\n",
    "\n",
    "    new_model = YOLO('c:/3rd_project/yolov8 pt/230925_a_1_best.pt')   # Mask 모델\n",
    "\n",
    "    # print(type(new_model.names), len(new_model.names))\n",
    "    # print(new_model.names)\n",
    "\n",
    "    '''\n",
    "    이미지 적용\n",
    "    '''\n",
    "    # # Define path to the image file\n",
    "    # # source = '/content/drive/MyDrive/images/movie/action/220.jpg'\n",
    "    # source = '/content/drive/MyDrive/images/movie/action/7.jpg'\n",
    "\n",
    "    results = list(new_model.predict(source='c:/3rd_project/data/images/movie/action/411.jpg',\n",
    "                                conf=0.3, show=True, stream=True))\n",
    "    #                             예측률0.5초과                      detect할 클래스만 classes=[0, 2, ...] 추가\n",
    "    #                                                                없는 경우 생략\n",
    "\n",
    "    # for r in results:\n",
    "    #   print(r)\n",
    "\n",
    "    '''\n",
    "    이미지 출력\n",
    "    '''\n",
    "    res_plotted = results[0].plot()\n",
    "    plt.imshow(cv2.cvtColor(res_plotted, cv2.COLOR_BGR2RGB))\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    '''\n",
    "    개체 수 출력\n",
    "    '''\n",
    "    # gpu => cpu\n",
    "    cls_cpu = results[0].boxes.cls.to('cpu').to(int)\n",
    "    # cpu => list\n",
    "    cls_list = cls_cpu.tolist()\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    cls_counts = Counter(cls_list)\n",
    "    # print(cls_counts)\n",
    "\n",
    "    # 클래스 이름과 개수를 연결하는 딕셔너리 생성\n",
    "    class_names = {\n",
    "        new_model.names[class_idx]: str(count)\n",
    "        for class_idx, count in cls_counts.items()\n",
    "    }\n",
    "\n",
    "    # 모든 키-값 쌍을 반복하고 출력\n",
    "    for class_name, class_value in class_names.items():\n",
    "        print(f\"Class: {class_name}\\t\\tValue: {class_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
